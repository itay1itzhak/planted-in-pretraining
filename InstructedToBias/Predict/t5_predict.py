import torch
import torch.nn.functional as F
import os
from transformers import (
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    T5Tokenizer,
    T5ForConditionalGeneration,
)

from Predict.hugging_face_perdictor import HFPredictor

sm = torch.nn.LogSoftmax(dim=1)
import logging

logging.basicConfig(level=logging.INFO)
logging.getLogger().setLevel(logging.INFO)
import os


class T5Predictor(HFPredictor):
    def get_scores_for_labels(self, input, labels):
        batch_size, num_labels = len(input), len(labels)
        # Get encodings
        input_enc = self.tokenizer.batch_encode_plus(
            input,
            return_tensors="pt",
            add_special_tokens=True,
            truncation=True,
            padding="longest",
        )
        target_enc = self.tokenizer.batch_encode_plus(
            labels, return_tensors="pt", padding="longest"
        ).input_ids

        for k, v in input_enc.items():
            input_enc[k] = v.to(self.model.device)
        target_enc = target_enc.to(self.model.device)

        # Get encoder's last hidden state
        encoder_hidden_states = self.model.encoder(**input_enc)[0]

        # Repeat the inputs `num_label` times
        encoder_hidden_states = (
            encoder_hidden_states.unsqueeze(dim=1)
            .repeat(1, num_labels, 1, 1)
            .flatten(0, 1)
        )
        attention_mask = (
            input_enc.attention_mask.unsqueeze(dim=1)
            .repeat(1, num_labels, 1)
            .flatten(0, 1)
        )

        # Create the decoding mask (that is commonly generated by the T5 model at predict time) -- makes it more efficient
        decoder_input_ids = torch.cat(
            [
                torch.zeros(
                    (num_labels * batch_size, 1),
                    dtype=torch.int,
                    device=self.model.device,
                ),
                target_enc[:, :-1].repeat(batch_size, 1),
            ],
            dim=1,
        )
        decoder_attention_mask = (decoder_input_ids == decoder_input_ids).float()
        lm_target = (
            target_enc - 100 * (target_enc == self.tokenizer.pad_token_id).long()
        )

        model_output = self.model(
            attention_mask=attention_mask,
            encoder_outputs=[encoder_hidden_states],
            decoder_input_ids=decoder_input_ids,
            decoder_attention_mask=decoder_attention_mask,
        )

        # Compute the log probabilities associated with each of the labels
        labels_log_probs = F.cross_entropy(
            model_output.logits.flatten(0, 1),
            lm_target.repeat(batch_size, 1).flatten(0, 1),
            reduction="none",
        )

        # Sum log probs for each of the (input, label) pair
        labels_scores = labels_log_probs.view(batch_size, num_labels, -1)
        # We don't want to sum over the sequence dimension, just get the last token score (not including eos token)
        # labels_scores = labels_scores.sum(dim=-1)
        labels_scores = labels_scores[:, :, -2]

        # Note: Label log probabilities are positive (due to the internals of pytorch's
        # cross entropy). To obtain the "logits", we need to multiply by -1.
        return labels_scores * -1

    def get_generated_prediction(self, prompt):
        inputs = self.tokenizer(prompt, return_tensors="pt")
        if torch.cuda.is_available():
            inputs = inputs.to(self.model.device)
        outputs = self.model.generate(
            **inputs,
            max_new_tokens=self.max_tokens,
            return_dict_in_generate=True,
            output_scores=True,
        )

        pred_text = self.tokenizer.batch_decode(
            outputs.sequences, skip_special_tokens=True
        )
        log_probs, tokens_id = sm(torch.cat(outputs["scores"], dim=0)).max(dim=1)
        logits = {id.item(): p.item() for id, p in zip(tokens_id, log_probs)}

        return pred_text[0].strip().strip("."), logits

    def load_model_and_tokenizer(
        self,
    ):
        model_name, _, cache_dir = self.set_device_and_cache_dir()

        if self.model_path is not None:
            model_name_to_load = self.model_path
        else:
            model_name_to_load = f"google/{model_name}"

        if "flan" in model_name:
            model = AutoModelForSeq2SeqLM.from_pretrained(
                model_name_to_load,
                cache_dir=cache_dir,
                torch_dtype=torch.float32,  # torch_dtype,
                low_cpu_mem_usage=True,
            )  # , device_map="auto"#, load_in_8bit=True
            # )
            tokenizer = AutoTokenizer.from_pretrained(model_name_to_load)
        else:  # vanilla t51.1
            model = T5ForConditionalGeneration.from_pretrained(
                model_name_to_load,
                cache_dir=cache_dir,
                torch_dtype=torch.float32,  # torch_dtype,
                low_cpu_mem_usage=True,
            )  # ,device_map='balanced')#, device_map="auto"#, load_in_8bit=True
            # )
            tokenizer = T5Tokenizer.from_pretrained(model_name_to_load)

        self.change_model_device(model)
        self.model = model
        self.tokenizer = tokenizer

    def t5_tulu_convert_to_chat_format(self, text, few_shots_texts=None):
        # adapted from the Tulu chat format from open instruct by AI2
        def create_prompt_with_tulu_chat_format(messages, eos="</s>"):
            formatted_text = ""
            for message in messages:
                if message["role"] == "system":
                    formatted_text += "<|system|>\n" + message["content"] + "\n"
                elif message["role"] == "user":
                    formatted_text += "<|user|>\n" + message["content"] + "\n"
                elif message["role"] == "assistant":
                    formatted_text += (
                        "<|assistant|>\n" + message["content"].strip() + eos + "\n"
                    )
                else:
                    raise ValueError(
                        "Tulu chat template only supports 'system', 'user' and 'assistant' roles. Invalid role: {}.".format(
                            message["role"]
                        )
                    )
            formatted_text += "<|assistant|>\n"
            return formatted_text

        messages = []
        if few_shots_texts is not None:
            for shot in few_shots_texts:
                messages.extend(
                    [
                        self.get_chat_format_one_side(shot["question"], "user"),
                        self.get_chat_format_one_side(shot["answer"], "assistant"),
                    ]
                )
        # Add the actual question text to the user as the last message
        messages.append(self.get_chat_format_one_side(text, "user"))
        prompt = create_prompt_with_tulu_chat_format(messages)

        return prompt
